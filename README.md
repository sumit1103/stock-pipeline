ğŸ“ˆ Dockerized Stock Data Pipeline with Airflow & PostgreSQL
ğŸš€ Project Overview

This project is an end-to-end, Dockerized data pipeline that automatically fetches stock market data using the yfinance API, orchestrates tasks using Apache Airflow, and stores the data in a PostgreSQL database for analytics.

The pipeline is fully automated, reproducible, and designed to demonstrate real-world data engineering concepts such as ingestion, orchestration, storage, and transformation.

ğŸ› ï¸ Tech Stack

Python

Apache Airflow (2.8.1) â€“ workflow orchestration

PostgreSQL â€“ data storage

Docker & Docker Compose â€“ containerization

yfinance â€“ stock market data source

SQL â€“ data validation & analytics

ğŸ“Š Data Description

The pipeline fetches OHLCV stock data:

Open

High

Low

Close

Volume

Supported Stock Symbols

MSFT (Microsoft)

AAPL (Apple)

GOOG (Google)

ğŸ§± Project Architecture
yfinance API
     â†“
Apache Airflow DAG
     â†“
PythonOperator
     â†“
PostgreSQL Database
     â†“
SQL Analytics Queries

ğŸ“ Project Structure
stock-data-pipeline/
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ stock_data_dag.py
â”‚       â””â”€â”€ fetch_stock_data.py
â”‚
â””â”€â”€ postgres/

âš™ï¸ How the Pipeline Works

Airflow DAG runs on a schedule (daily).

DAG executes a PythonOperator.

Python script fetches historical stock data using yfinance.

Data is stored in PostgreSQL with a composite primary key to avoid duplicates.

SQL queries are used to validate and analyze the data.

Logs and retries are handled by Airflow.

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Prerequisites

Docker Desktop installed and running

Git installed

2ï¸âƒ£ Clone the Repository
git clone https://github.com/sumit1103/Dockerized-Data-Pipeline-with-Airflow.git
cd Dockerized-Data-Pipeline-with-Airflow

3ï¸âƒ£ Build & Start the Services
docker compose up --build

4ï¸âƒ£ Access Airflow UI

Open browser:

http://localhost:8000


Login Credentials

Username: airflow
Password: airflow

5ï¸âƒ£ Trigger the DAG (Optional)
docker compose exec airflow-webserver airflow dags trigger stock_data_pipeline

ğŸ—„ï¸ Database Validation
Connect to PostgreSQL
docker compose exec postgres psql -U airflow -d airflow

Verify Raw Data
SELECT * FROM stock_prices LIMIT 10;

Example Analytical Query
SELECT
  symbol,
  DATE(timestamp) AS trade_date,
  AVG(close) AS avg_close
FROM stock_prices
GROUP BY symbol, DATE(timestamp)
ORDER BY trade_date DESC;

ğŸ” Data Integrity

Uses (symbol, timestamp) as PRIMARY KEY

Prevents duplicate records with ON CONFLICT DO NOTHING

Airflow retries tasks on failure

â— Why yfinance?

Free and widely used

No API key required

No rate-limit exhaustion

Suitable for historical stock data

âœ… Key Features

Fully automated ETL pipeline

Dockerized & reproducible

Handles multiple stock symbols

Production-style Airflow setup

Clean GitHub repository (logs ignored)

ğŸ“Œ Future Enhancements (Optional)

Data visualization dashboard

Alerts on DAG failures

Technical indicators (SMA, EMA)

Cloud deployment

ğŸ¯ Conclusion

This project demonstrates a complete data engineering workflow:

ingestion â†’ orchestration â†’ storage â†’ analytics

It is suitable for internships, fresher roles, and junior data engineering positions.

ğŸ‘¤ Author

Sumit
GitHub: https://github.com/sumit1103
